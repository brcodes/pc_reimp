## Config file for PCC (r/s) experiments
## Parsed downstream by literal_eval (ast)
## Parameters are simple python types
## Parameters relevant for sPCC but not rPCC will be ignored for the latter, and vice versa.
## You can keep them all in, as long as your model_type is set correctly. rPCC not fully written. sPCC only.

## METADATA AND EXPERIMENT
# Will append model filenames
exp_name='architecture tests'
# Extra text about the experiments.
notes='architecture tests'

# Don't change:
train_with=True
evaluate_with=False
predict_with= False

## PCC MODEL
# Must be a tuple. num_tiles, flat_tile_length
input_shape=(16,864)

#tiles per image
num_tiles=16

# Total layers ('0th', ie input does not count. only include hidden plus output). Use >=3 only.
num_layers=3

# Hidden layer size. if tiled input, (num tiles, hidden_lyr_sizes[0]) == actual r1 array size.
# If 1-layer model, set hidden_lyr_sizes=[size1] and output_lyr_size=size1
# If 2-layer model, set hidden_lyr_sizes=[size1] and output_lyr_size=size2
# If 3 or more layers, set hidden_lyr_sizes=[size_i..., size_n-1] and output_lyr_size=size_n
hidden_lyr_sizes=[32,128]

# Output (n) layer size. 
# If classif_method None or 'c2', can be any size
# if 'c1', this must == number of classes
output_lyr_size=212

# sPCC Architecture (some may be applicable to rpcc down the line)
# i. all r's are flat (1d) (Rogers idea- an ML convention, but not as biologically equatable according to RB 1999) ('flat_hidden_lyrs')
# ii. expand r1 (2d) using Li architecture ('expand_first_lyr_Li') - uses exact same update equation components as her sPCC, leading to odd shapes, but a functioning model (212 ps classification)
# iii. expand r1 (2-3d) using Rogers architecture ('expand_first_lyr') - how an expansion probably should have worked, to keep sizes consistent, math sensible, incl. no online reshaping (slow).
architecture='expand_first_lyr_Li'

# C1 or C2, see Rogers/Brown work for clarification; can be None, 'c1', 'c2'. None should be supported (sets terms to 0)
classif_method='c1'

# gaussian is standard normal, kurtotic is sparse kurtotic from a laplacian, Li_priors is Li method: rs are 0s, Us are uniform random in [-0.5,0.5]
priors='Li_priors'

# learning rate denominators for cost function 
# Brown math had them as 2 for priors (what it should be for derivatives of functions with square term) and rn top-down update terms ('Brown_denominators')
# Li math had them as 1 (no change to lr) for priors, and ssq_n for rn top-down update terms. 'Li_denominators'
# Other denominators are the same between the two systems: BU updates take ssq(i-1) and TD updates take ssq(i)
lr_denominators='Li_denominators'

# Update method: now rs and Us are updated, and when. This system has been designed to work with V's, as well.
# Li method, rW (Weights) updated together (r1-n then U1-n) for 30 iters per image: {'rW_niters':30}
# Rogers/Brown proposal #1: r updated (1-n) for 100 iters per image before W update (U1-n) {'r_niters_W':100}
# Rogers/Brown proposal #2. rs updated until change in ea. indiv. vector < some stop criterion per image, then U or V  {'r_eq_W':0.05}
update_method={'rW_niters':30}

# softmax (normal or stable)
softmax_type='normal'
softmax_k=1

# Don't change:
# Can be static or recurrent
model_type='static'
# Can be tiled or not
tiled_input=True
# Flat or not.
flat_input=True
# Tanh or linear
activ_func='linear'

## TRAINING DATASET
num_imgs=212
num_classes=212
# This should be our reproduction of her dataset
# shape: 3392,864 *** (see below)
# or: 212 (n) imgs x 16 (n) tiles per image, size of one flat tile (A) (864) *** (see below)
# this is reshaped in model.train() to be 212,16,864. Then, each 16,864 image is fed to the model.
# You can change this dataset, the name doesn't matter. Just make sure model gets something tiled, in the shape format above *** (see above)
dataset_train='ds.trace212_212_li_trace212_132_84_tl_16_36_24_32_20.pydb'

## TRAINING HYPERPARAMETERS
# how many exposures to all of the data (shuffled)
epoch_n=1

# learning rates r, one per layer plus o needs to be correct if classif_method == 'c2'
# Note that Li learning rate k1 == kr, and she had one k1 for all layers. 
kr={1:0.001, 2:0.001, 3:0.001, 'o':0.001}

# learning rates U, one per layer plus o needs to be correct if classif_method == 'c2'
# Note that Li learning rate k2 == kU, and she had one k2 for all layers.
# Consistency with her method means all kr need to be the same for each layer, all kU " ".
kU={1:0.001, 2:0.001, 3:0.001, 'o':0.001}

# prior distribution parameters, sPCC only. one per layer, incl. Uo (c2) case (alpha for r, lambda for U)
alph= {1:1.0, 2:0.05, 3:0.05} 
lam= {1:0.001, 2:0.001, 3:0.001, 'o':0.001}

# layer covariance parameters, sigma squared.
ssq= {0:1, 1:10, 2:10, 3:2} 

# plot loss and accuracy by epoch directly after training
# If you're running something on the cluster, it may not work unless you turn this off, comment out matplotlib.
plot_train= True 

# Don't change (unless you add batch support. Li's should be stochastic, though.):
# Stochastic gradient descent is 1, all above is batch gd.
batch_size=1

# Don't change:
# save checkpoints? None, save every ('save_every') Int or every 'fraction' (Float, 1/N) of total epochs
# The only thing that differentiates a 'checkpoint' from a 'model' is abstractly the latter is considered fully trained, while the former is not
# Under this paradigm, models are saved in models/ and checkpoints are saved in models/checkpoints/
save_checkpoint= {'save_every':50} 

# if checkpoints present, load? None, or Int != -1 for checkpoint epoch desired. Int == -1 for latest checkpoint.
load_checkpoint=None


