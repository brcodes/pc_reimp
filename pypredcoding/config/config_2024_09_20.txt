## Config file for PCC (r/s) experiments
## Parsed downstream by literal_eval (ast)
## Parameters are simple python types
## Parameters relevant for sPCC but not rPCC will be ignored for the latter, and vice versa.
## You can keep them all in, as long as your model_type is set correctly. rPCC not fully written. sPCC only.

## METADATA AND EXPERIMENT

# Will append model filenames
exp_name='li_params_prior_dists_r1U2'
# Extra text about the experiments.
notes='2024_09_20 r1 16,32 U2 512,128, param equiv., prior dists - r100W success exploration'

# Don't change:
train_with=True
evaluate_with=False
predict_with= False

## PCC MODEL

# Must be a tuple. num_tiles, flat_tile_length
input_shape=(16,864)
#tiles per image
num_tiles=16
# Total layers ('0th', ie input does not count. only include hidden plus output). Use >=3 only.
num_layers=3
# Hidden layer size. if tiled input, (num tiles, hidden_lyr_sizes[0]) == actual r1 array size.
# If 1-layer model, set hidden_lyr_sizes=[size1] and output_lyr_size=size1
# If 2-layer model, set hidden_lyr_sizes=[size1] and output_lyr_size=size2
# If 3 or more layers, set hidden_lyr_sizes=[size_i..., size_n-1] and output_lyr_size=size_n
hidden_lyr_sizes=[32,128]
# Output (n) layer size. 
# If classif_method None or 'c2', can be any size
# if 'c1', this must == number of classes
output_lyr_size=212
# sPCC Architecture (some may be applicable to rpcc down the line)
# i. all r's are flat (1d) (Rogers idea- an ML convention, but not as biologically equatable according to RB 1999) ('flat_hidden_lyrs')
# ii. expand r1 (2d) using Li architecture ('expand_first_lyr_Li') - uses exact same update equation components as her sPCC, leading to odd shapes, but a functioning model (212 ps classification)
# iii. expand r1 (2d) using Rogers architecture ('expand_first_lyr') - how an expansion probably should have worked, to keep sizes consistent, math sensible, incl. no online reshaping (slow).
architecture='expand_first_lyr_Li'
# C1 or C2, see Rogers/Brown work for clarification; can be None, 'c1', 'c2'. None should be supported (sets terms to 0)
classif_method='c1'
# Gaussian is standard normal,  kurtotic is Li method: rs are 0s, Us are uniform random in [-0.5,0.5]
priors='Li_priors'
# learning rate denominators for cost function prior terms
# Brown math had them as 2 (what it should be for derivatives of functions with square terms) 'Brown_denominators'
# Li math had them as 1 (no change to lr). 'Li_denominators'
# Other denominators are the same between the two systems: BU updates take ssq(i-1) and TD updates take ssq(i)
lr_prior_denominator='Li_denominators'
# Update method: now rs and Us are updated, and when. This system has been designed to work with V's, as well.
# Li method, rW (Weights) updated together (r1-n then U1-n) for 30 iters per image: {'rW_niters':30}
# Rogers/Brown proposal #1: r updated (1-n) for 100 iters per image before W update (U1-n) {'r_niters_W':100}
# Rogers/Brown proposal #2. rs updated until change in ea. indiv. vector < some stop criterion per image, then U or V  {'r_eq_W':0.05}
update_method={'r_niters_W':100}

# Don't change:
# Can be static or recurrent
model_type='static'
# Can be tiled or not
tiled_input=True
# Flat or not.
flat_input=True
# Tanh or linear
activ_func='linear'

## TRAINING DATASET

num_imgs=212
num_classes=212
# This should be our reproduction of her dataset
# shape: 3392,864 *** (see below)
# or: 212 (n) imgs x 16 (n) tiles per image, size of one flat tile (A) (864) *** (see below)
# this is reshaped in model.train() to be 212,16,864. Then, each 16,864 image is fed to the model.
# You can change this dataset, the name doesn't matter. Just make sure model gets something tiled, in the shape format above *** (see above)
dataset_train='ds.trace212_212_li_trace212_132_84_tl_16_36_24_32_20.pydb'

## TRAINING HYPERPARAMETERS
# how many exposures to all of the data (shuffled)
epoch_n=1000
# learning rates r, one per layer plus o needs to be correct if classif_method == 'c2'
# Note that Li learning rate k1 == kr, and she had one k1 for all layers. 
kr={1:0.001, 2:0.001, 3:0.001, 'o':0.001}
# learning rates U, one per layer plus o needs to be correct if classif_method == 'c2'
# Note that Li learning rate k2 == kU, and she had one k2 for all layers.
# Consistency with her method means all kr need to be the same for each layer, all kU " ".
kU={1:0.001, 2:0.001, 3:0.001, 'o':0.001}
# prior distribution parameters, sPCC only. one per layer, incl. Uo (c2) case (alpha for r, lambda for U)
alph= {1:1.0, 2:0.05, 3:0.05} 
lam= {1:0.001, 2:0.001, 3:0.001, 'o':0.001}
# layer covariance parameters, sigma squared.
ssq= {0:1, 1:10, 2:10, 3:2, 'o':2} 

# Don't change (unless you add batch support. Li's should be stochastic, though.):
# Stochastic gradient descent is 1, all above is batch gd.
batch_size=1
# Don't change:
online_diagnostics= True



