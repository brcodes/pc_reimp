## Config file for PCC (r/s) experiments
## Parsed downstream by literal_eval (ast)
## Parameters are simple python types
## Parameters relevant for sPCC but not rPCC will be ignored for the latter, and vice versa.
## You can keep them all in, as long as your model_type is set correctly. rPCC not fully written. sPCC only.

## METADATA AND EXPERIMENT

# Will append model filenames
exp_name='li_params_prior_dists_r1U2'
# Extra text about the experiments.
notes='2024_09_20 r1 16,32 U2 512,128, param equiv., prior dists - r100W success exploration'

# Don't change:
train_with=True
evaluate_with=False
predict_with= False

## PCC MODEL

# Must be a tuple. num_tiles, flat_tile_length
input_size=(16,864)
#tiles per image
num_tiles=16
# Total layers ('0th', ie input does not count. only include hidden plus output). Use >=3 only.
num_layers=3
# Hidden layer size. if tiled input, (num tiles, hidden_lyr_sizes[0]) == actual r1 array size.
hidden_lyr_sizes=[32,128]
# Output (n) layer size. If classif_method None or 'c2', can be any size; if 'c1', this must == number of classes
output_lyr_size=212
# C1 or C2, see Rogers/Brown work for clarification; can be None, 'c1', 'c2'. None should be supported (sets terms to 0)
classif_method='c1'
# Gaussian is standard normal,  kurtotic is Li method: rs are 0s, Us are uniform random in [-0.5,0.5]
priors='kurtotic'
# priors='Li_priors'
# Update method: now rs and Us are updated, and when. This system has been designed to work with V's, as well.
# Li method, rW (Weights) updated together (r1-n then U1-n) for 30 iters per image: {'rW_niters':30}
# Rogers/Brown proposal #1: r updated (1-n) for 100 iters per image before W update (U1-n) {'r_niters_W':100}
# Rogers/Brown proposal #2. rs updated until change in ea. indiv. vector < some stop criterion per image, then U or V  {'r_eq_W':0.05}
update_method={'r_niters_W':100}

# Don't change:
# Can be static or recurrent
model_type='static'
# Can be tiled or not
tiled=True
# Flat or not.
flat_input=True
# Tanh or linear
activ_func='linear'

## TRAINING DATASET

num_imgs=212
num_classes=212
# This should be our reproduction of her dataset
# shape: 3392,864 *** (see below)
# or: 212 imgs x 16 tiles per image, size of one flat tile (864) *** (see below)
# this is reshaped in model.train() to be 212,16,864. Then, each 16,864 image is fed to the model.
# You can change this dataset, the name doesn't matter. Just make sure model gets something tiled, in the shape format above ***
dataset_train='ds.trace212_212_li_trace212_132_84_tl_16_36_24_32_20.pydb'

## TRAINING HYPERPARAMETERS
# how many exposures to all of the data (shuffled)
epoch_n=1000
# learning rates r, one per layer plus o needs to be correct if classif_method == 'c2'
kr={1:0.001, 2:0.001, 3:0.001, 'o':0.001}
# learning rates U, one per layer plus o needs to be correct if classif_method == 'c2'
kU={1:0.001, 2:0.001, 3:0.001, 'o':0.001}
# prior distribution parameters, sPCC only. one per layer, incl. Uo (c2) case (alpha for r, lambda for U)
alph= {1:1.0, 2:0.05, 3:0.05} 
lam= {1:0.001, 2:0.001, 3:0.001, 'o':0.001}
# layer covariance parameters, sigma squared.
ssq= {0:1, 1:10, 2:10, 3:2, 'o':2} 

# Don't change (unless you add batch support. Li's should be stochastic, though.):
# Stochastic gradient descent is 1, all above is batch gd.
batch_size=1
# Don't change:
online_diagnostics= True



