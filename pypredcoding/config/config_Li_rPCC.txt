## Config file for PCC (r/s) experiments
## Parsed downstream by literal_eval (ast)
## Parameters are simple python types
## Parameters relevant for sPCC but not rPCC will be ignored for the latter, and vice versa.
## You can keep them all in, as long as your model_type is set correctly. rPCC not fully written. sPCC only.

## ********
## Note for Kevin
## ********

# While Li_priors and Li learning rate denominators will work with models of any type
# Li architecture 'expand_first_lyr_Li' includes her funky update math, which will probably not.
# This config is designed for a strict replication of her model params, only
# i.e. n = 3 layers, tiled_input = true, flat_input = true.

## ********

## ********

## METADATA AND EXPERIMENT
# Will append model filenames
exp_name='rpcc start tests 12 cvcv'
# Extra text about the experiments.
notes='rpcc start tests 12 cvcv'

# Don't change:
train_with=True
evaluate_with=False
predict_with= False

## PCC MODEL
# Must be a tuple.
input_shape=(21,)

#tiles per image
num_tiles=None

# Total layers ('0th', ie input does not count. only include hidden plus output). Use >=3 only.
num_layers=2

# Hidden layer size. if tiled input, (num tiles, hidden_lyr_sizes[0]) == actual r1 array size.
# If 1-layer model, set hidden_lyr_sizes=[size1] and output_lyr_size=size1
# If 2-layer model, set hidden_lyr_sizes=[size1] and output_lyr_size=size2
# If 3 or more layers, set hidden_lyr_sizes=[size_i..., size_n-1] and output_lyr_size=size_n
hidden_lyr_sizes=[10]

# Output (n) layer size. 
# If classif_method None or 'c2', can be any size
# if 'c1', this must == number of classes
output_lyr_size=12

# Keep r_context (r2hatx) or not- Rogers model only. Li comes with it no matter what.
rc=True

# C1 or C2, see Rogers/Brown work for clarification; can be None, 'c1', 'c2'. None should be supported (sets terms to 0)
classif_method='c1'

# gaussian is standard normal, kurtotic is sparse kurtotic from a laplacian, Li_gaussian gaussian loc 0, scale 0.1
priors='Li_gaussian'

# Update method: now rs and Us are updated, and when. This system has been designed to work with V's, as well.
# Li method, spcc: rW (Weights) updated together (r1-n then U1-n) for 30 iters per image: {'rW_niters':30}
# Li method, rpcc: rW (Weights) updated together (r1-n then U1-n) for 1 iters per image: {'rW_niters':1} (This is done once each timestep)
# Rogers/Brown proposal #1: r updated (1-n) for 100 iters per image before W update (U1-n) {'r_niters_W':100}
# Rogers/Brown proposal #2. rs updated until change in ea. indiv. vector < some stop criterion per image, then U or V  {'r_eq_W':0.05}
update_method={'rW_niters':1}

# softmax (normal or stable)
softmax_type='stable'
# Li's rpcc had k 20 .
softmax_k=20

# Don't change:
# Can be static or recurrent
model_type='recurrent'
# Can be tiled or not
tiled_input=False
# Flat or not.
flat_input=True
# Tanh or linear
activ_func='linear'
# Recurrent only, will ignore otherwise
num_ts=98

## TRAINING DATASET
num_inps=12
num_classes=12
dataset_train='Li_cvcv12.pkl'

## TRAINING HYPERPARAMETERS
# how many exposures to all of the data (shuffled)
# 1000 should be at 60% accuract.
epoch_n=1000


# learning rates r, one per layer plus o needs to be correct if classif_method == 'c2'
# Note that Li learning rate k1 == kr, and she had one k1 for all layers. 
kr={1:0.001, 2:0.001, 3:0.001, 'o':0.001}

# learning rates U, one per layer plus o needs to be correct if classif_method == 'c2'
# Note that Li learning rate k2 == kU, and she had one k2 for all layers.
# Consistency with her method means all kr need to be the same for each layer, all kU " ".
kU={1:0.001, 2:0.001, 3:0.001, 'o':0.001}

# V recurrent weights
kV={1:0.001, 2:0.001, 3:0.001, 'o':0.001}

# layer covariance parameters, sigma squared.
ssqr= {0:1, 1:10, 2:10, 3:2} 
# for V
ssqV= {0:1, 1:10, 2:10, 3:2} 

# plot loss and accuracy by epoch directly after training
# If you're running something on the cluster, it may not work unless you turn this off, comment out matplotlib, plot code.
# This plots during hard-coded "every 10 epochs diagnostics" for KB as well.
plot_train= True 

# save checkpoints? None (no) 
# save every N {'save_every': Int}
# save every 1/N of total epochs {'fraction': 0.1} or {'fraction': (1/10)}
# The only thing that differentiates a 'checkpoint' from a 'model' is abstractly the latter is considered fully trained, while the former is not
# Under this paradigm, models are saved in models/ and checkpoints are saved in models/checkpoints/
save_checkpoint= {'save_every':5} 

# if checkpoints present, load? None for none. Int != -1 for checkpoint epoch desired. Int == -1 for latest checkpoint.
load_checkpoint=None

# Don't change (unless you add batch support. Li's should be stochastic, though):
# Stochastic gradient descent is 1, all above is batch gd.
batch_size=1



